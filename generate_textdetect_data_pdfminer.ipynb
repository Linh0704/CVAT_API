{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import uuid\n",
    "import logging\n",
    "import getpass\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import imageio\n",
    "import random\n",
    "import copy\n",
    "import glob\n",
    "import json\n",
    "import shutil\n",
    "import pickle\n",
    "import traceback\n",
    "from collections import Counter\n",
    "from pprint import pprint as print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from PIL import Image, ImageDraw\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import requests\n",
    "import pdf2image\n",
    "import pdfminer\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage, PDFTextExtractionNotAllowed\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.layout import LAParams, LTTextBoxHorizontal, LTTextContainer, LTLayoutContainer, LTTextLineHorizontal, LTAnno, LTChar\n",
    "from pdfminer.converter import PDFPageAggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, figsize=(20, 20)):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    ax.axis('off')\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_num = 3\n",
    "\n",
    "def cut_text(string, word_num):\n",
    "    points = [0]\n",
    "    w_n = 0\n",
    "    for c, char in enumerate(string):\n",
    "        if c == 0 and char.isspace():\n",
    "            continue\n",
    "        if c == len(string) - 1:\n",
    "            if not char.isspace():\n",
    "                w_n += 1\n",
    "                if w_n >= min_word_num:\n",
    "                    points.append(c)\n",
    "            if char.isspace() and w_n >= min_word_num and w_n < word_num:\n",
    "                points.append(c)\n",
    "            else:\n",
    "                continue\n",
    "        if not char.isspace() and string[c + 1].isspace():\n",
    "            w_n += 1\n",
    "        if w_n == word_num:\n",
    "            points.append(c)\n",
    "            w_n = 0\n",
    "    word_batches = []\n",
    "    for p, point in enumerate(points):\n",
    "        if p == 0:\n",
    "            continue\n",
    "        start = points[p - 1] if p == 1 else points[p - 1] + 1\n",
    "        end = point\n",
    "        for i in range(start, end + 1):\n",
    "            if not string[i].isspace():\n",
    "                start = i\n",
    "                break\n",
    "        for i in range(end, start - 1, -1):\n",
    "            if not string[i].isspace():\n",
    "                end = i\n",
    "                break\n",
    "        word_batches.append([start, end])\n",
    "    return word_batches\n",
    "\n",
    "def word_batches_in_line(line, word_num, page_height):\n",
    "    result_word_batches = []\n",
    "    text = line.get_text()\n",
    "    word_batches = cut_text(text, word_num)\n",
    "    for word_batch in word_batches:\n",
    "        [start, end] = word_batch\n",
    "        label = normalize(\"NFC\", \"\")\n",
    "        for w, word in enumerate(line):\n",
    "            if w == start:\n",
    "                char_start = word\n",
    "            if w == end:\n",
    "                char_end = word\n",
    "            if w >= start and w <= end:\n",
    "                label += normalize(\"NFC\", word.get_text())\n",
    "        sx0, sy0, sx1, sy1 = list(map(lambda i: int(i), [char_start.x0, char_start.y0, char_start.x1, char_start.y1]))\n",
    "        sx_min = int(min(sx0, sx1))\n",
    "        sy_min = int(min(page_height - sy0, page_height - sy1)) - 3\n",
    "        sx_max = int(max(sx0, sx1))\n",
    "        sy_max = int(max(page_height - sy0, page_height - sy1))\n",
    "\n",
    "        ex0, ey0, ex1, ey1 = list(map(lambda i: int(i), [char_end.x0, char_end.y0, char_end.x1, char_end.y1]))\n",
    "        ex_min = int(min(ex0, ex1))\n",
    "        ey_min = int(min(page_height - ey0, page_height - ey1)) - 3\n",
    "        ex_max = int(max(ex0, ex1))\n",
    "        ey_max = int(max(page_height - ey0, page_height - ey1))\n",
    "\n",
    "        wx_min = min(sx_min, ex_min)\n",
    "        wy_min = min(sy_min, ey_min)\n",
    "        wx_max = max(sx_max, ex_max)\n",
    "        wy_max = max(sy_max, ey_max)\n",
    "\n",
    "        result_word_batches.append([[wx_min, wy_min, wx_max, wy_max], label])\n",
    "    return result_word_batches\n",
    "\n",
    "def aug_line_text(line, page_height):\n",
    "    text = line.get_text()\n",
    "    num_words_in_line = len(text.split())\n",
    "    result_aug_line_text = []\n",
    "    if num_words_in_line < min_word_num:\n",
    "        return result_aug_line_text\n",
    "\n",
    "    for number in range(min_word_num, num_words_in_line + 1):\n",
    "        word_batches_with_num = word_batches_in_line(line, number, page_height)\n",
    "        for word_batch in word_batches_with_num:\n",
    "            if word_batch not in result_aug_line_text:\n",
    "                result_aug_line_text.append(word_batch)\n",
    "    return result_aug_line_text\n",
    "\n",
    "def line_process(img, lines, base_file_name, page_height):\n",
    "    for l, line in enumerate(lines):\n",
    "        if not normalize(\"NFC\", line.get_text()):\n",
    "            continue\n",
    "\n",
    "        if l == 0:\n",
    "            info_phrases = aug_line_text(line, page_height)\n",
    "            for i, info_phrase in enumerate(info_phrases):\n",
    "                [[x_min, y_min, x_max, y_max], label] = info_phrase\n",
    "                try:\n",
    "                    crop_img = img[y_min:y_max, x_min:x_max]\n",
    "                    aug_imgs = [crop_img] + augmentation(crop_img)\n",
    "                    new_file_name = base_file_name + \"_line\" + str(l) + \"_phr\" + str(i)\n",
    "                    for a, aug_img in enumerate(aug_imgs):\n",
    "                        file_name = new_file_name + \"_aug\" + str(a) + \".png\"\n",
    "                        cv2.imwrite(file_name, aug_img)\n",
    "                        annotation_file.write(file_name + \"\\t\" + label + \"\\n\")\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "def paragraphs_process(img, paragraphs, index_page, pdf_path, page_height, scanned = False):\n",
    "    pdf_file_name = pdf_path.split(\"/\")[-1][:-4]\n",
    "    pdf_file_name = pdf_file_name + \"_scanned\" if scanned else pdf_file_name\n",
    "    for p, paragraph in enumerate(paragraphs):\n",
    "        base_file_name = \"./crop_images/\" + pdf_file_name + \"_page\" + str(index_page) + \"_para\" + str(p)\n",
    "        line_process(img, paragraph, base_file_name, page_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coords(obj):\n",
    "    \"\"\"convert pdfminer's coordinate to the standard format (from top-left)\n",
    "    \"\"\"\n",
    "#     sx0, sy0, sx1, sy1 = list(map(int, [char.x0, char.y0, char.x1, char.y1]))\n",
    "    try:\n",
    "        sx0, sy0, sx1, sy1 = obj.x0, obj.y0, obj.x1, obj.y1\n",
    "    except Exception:\n",
    "        sx0, sy0, sx1, sy1 = obj\n",
    "    sx_min = min(sx0, sx1)\n",
    "    sy_min = min(page_height - sy0, page_height - sy1)\n",
    "    sx_max = max(sx0, sx1)\n",
    "    sy_max = max(page_height - sy0, page_height - sy1)\n",
    "    \n",
    "    y_delta = sy_max - sy_min\n",
    "    \n",
    "    return sx_min, sy_min, sx_max, sy_max\n",
    "\n",
    "class Space:\n",
    "    fontname = None\n",
    "    def get_text(self):\n",
    "        return \" \"\n",
    "\n",
    "def get_info_LT(paragraphs):\n",
    "    lines, chars = [], []\n",
    "    for textbox in paragraphs:\n",
    "        if isinstance(textbox, pdfminer.layout.LTTextBoxHorizontal):\n",
    "            for textline in textbox:  # lines\n",
    "                if isinstance(textline, pdfminer.layout.LTTextLineHorizontal):\n",
    "                    textline_content = textline.get_text().strip()\n",
    "                    if textline_content:\n",
    "                        lines.append(textline)\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    for char in textline:\n",
    "#                         text_content = char.get_text().strip()\n",
    "                        if isinstance(char, pdfminer.layout.LTChar):\n",
    "#                             print(repr(text_content))\n",
    "#                             print(char.fontname)\n",
    "                            chars.append(char)\n",
    "                         \n",
    "                # define rule to ignore merged lines\n",
    "                chars.append(Space())\n",
    "\n",
    "    return lines, chars\n",
    "\n",
    "def get_words(chars, text_chars, debug=False):\n",
    "    \n",
    "#     text_chars = \"\".join([i.get_text() for i in chars])\n",
    "    words, tmp = [], []\n",
    "    for i, char in enumerate(text_chars):\n",
    "#         if char.strip():\n",
    "        if len(char.strip()) > 0:\n",
    "            tmp.append(chars[i])\n",
    "#             tmp.append(char)\n",
    "        else:\n",
    "            if tmp:\n",
    "                if debug:\n",
    "                    print(repr(\"\".join([i.get_text() for i in tmp])))\n",
    "                words.append(tmp)\n",
    "            tmp = []\n",
    "\n",
    "    if tmp:\n",
    "        words.append(tmp)\n",
    "    return words\n",
    "\n",
    "def get_xyxy_from_LT(obj):\n",
    "    x_min = min(i.x0 for i in obj)\n",
    "    y_min = min(i.y0 for i in obj)\n",
    "    x_max = max(i.x1 for i in obj)\n",
    "    y_max = max(i.y1 for i in obj)\n",
    "    return x_min, y_min, x_max, y_max\n",
    "\n",
    "def expand_box(coord, h_scale=0.16, to_int=True):\n",
    "    x1, y1, x2, y2 = coord\n",
    "    h = y2 - y1\n",
    "    y_delta = h * h_scale\n",
    "    y1 = y1 - y_delta\n",
    "    if to_int:  # for visualize\n",
    "        x1, y1, x2, y2 = map(int, (x1, y1, x2, y2))\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def scale_coord(coord, x_ratio, y_ratio):\n",
    "    x1, y1, x2, y2 = coord\n",
    "    \n",
    "    x1, x2 = x1 * x_ratio, x2 * x_ratio\n",
    "    y1, y2 = y1 * y_ratio, y2 * y_ratio\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def check_same_h(obj, thresh=2):\n",
    "    # ?\n",
    "    all_y0 = set([round(i.y0, 1) for i in obj])\n",
    "    all_y1 = set([round(i.y1, 1) for i in obj])\n",
    "    if len(all_y0) == 1 and len(all_y1) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "all_minus = [\"–\", \"—\", \"-\", \"+\", \":\"]\n",
    "all_dots = [\"…\", \".\"]\n",
    "def process_special_cases(obj, coord, h_scale=0.16, debug=False, to_int=True):\n",
    "\n",
    "    _texts = \"\".join(i.get_text() for i in obj)\n",
    "    if debug:\n",
    "        if len(_texts) == 1 and _texts not in text_vocab:\n",
    "            print(\"{} : {}\".format(_texts, ord(_texts)))\n",
    "\n",
    "    is_same_h = check_same_h(obj)\n",
    "    if not is_same_h:\n",
    "        if debug:\n",
    "            print(\"{} : {}\".format(_texts, len(_texts)))\n",
    "            print([i.y0 for i in obj])\n",
    "            print([i.y1 for i in obj])\n",
    "        h_scale = 0.01\n",
    "        \n",
    "    if _texts in all_minus:\n",
    "        h_scale = 0.0\n",
    "        \n",
    "    x1, y1, x2, y2 = expand_box(coord, h_scale=h_scale, to_int=to_int)\n",
    "\n",
    "    # list minus chars\n",
    "    # – - \n",
    "    if any(i in _texts for i in all_minus) and len(set(_texts)) == 1:\n",
    "        # for `-`\n",
    "        y_center = (y1 + y2) / 2\n",
    "        y1 = (y1 + y_center) / 2\n",
    "        y2 = (y2 + y_center) / 2\n",
    "        y1 = int(y1)\n",
    "        y2 = int(y2)\n",
    "    elif any(i in _texts for i in all_dots) and len(set(_texts)) == 1:\n",
    "        # for `.......`\n",
    "        y1 += (y2 - y1) / 2\n",
    "        y1 = int(y1)\n",
    "\n",
    "    return x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvpl_fd = \"/media/SUN-ASTERISK\\phan.huy.hoang/My Passport/phanhoang/data/PDF_converter/thuvienphapluat/pdf/\"\n",
    "pdf_fps = sorted(glob.glob(os.path.join(tvpl_fd, \"*.pdf\")))\n",
    "print(len(pdf_fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = random.choice(pdf_fps)\n",
    "print(pdf_path)\n",
    "assert os.path.exists(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pdf_path, \"rb\") as open_pdf:\n",
    "\n",
    "    # pdf to images\n",
    "    images = pdf2image.convert_from_path(pdf_path)\n",
    "    print(len(images))\n",
    "\n",
    "    # define pdfminer\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    laparams = LAParams()\n",
    "    device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    parser = PDFParser(open_pdf)\n",
    "    document = PDFDocument(parser)\n",
    "\n",
    "    # loop for pages\n",
    "    for p, page in enumerate(PDFPage.get_pages(open_pdf)):\n",
    "        try:\n",
    "            interpreter.process_page(page)\n",
    "            paragraphs = device.get_result()\n",
    "\n",
    "            [x_min, y_min, x_max, y_max] = page.mediabox\n",
    "            page_height, page_width = int(abs(y_max - y_min)), int(abs(x_max - x_min))\n",
    "\n",
    "            origin_img = np.array(images[p])\n",
    "            h_origin, w_origin, _ = origin_img.shape\n",
    "            x_ratio = h_origin / page_height\n",
    "            y_ratio = w_origin / page_width\n",
    "\n",
    "            beauty_img = cv2.resize(origin_img.copy(), (page_width, page_height))\n",
    "#             scanned_img = scanned_augmentation(beauty_img.copy())\n",
    "#             if p == 4:\n",
    "#                 break\n",
    "            break\n",
    "\n",
    "#             paragraphs_process(beauty_img, paragraphs, p, pdf_path, page_height)\n",
    "#             paragraphs_process(scanned_img, paragraphs, p, pdf_path, page_height, True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_origin, w_origin, _ = np.array(images[0]).shape\n",
    "y_ratio = h_origin / page_height\n",
    "x_ratio = w_origin / page_width\n",
    "print(x_ratio)\n",
    "print(y_ratio)\n",
    "print(origin_img.shape)\n",
    "print(beauty_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(origin_img, (20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize word box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(glob.glob(\"/media/SUN-ASTERISK\\phan.huy.hoang/My Passport/phanhoang/data/PDF_converter/vanban.chinhphu.vn/pdf/*.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_length_special_cases(chars):\n",
    "    text_chars = []\n",
    "    for i in chars:\n",
    "        _text = i.get_text()\n",
    "        if len(_text) > 1:  # some chars have length > 1\n",
    "            _text = _text[0]\n",
    "        text_chars.append(_text)\n",
    "    text_chars = \"\".join(text_chars)\n",
    "    return text_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1 page\n",
    "lines, chars = get_info_LT(paragraphs)\n",
    "# text_chars = \"\".join([i.get_text() for i in chars])\n",
    "text_chars = fix_length_special_cases(chars)\n",
    "print(\"{} : {}\".format(len(chars), len(text_chars)))\n",
    "words = get_words(chars, text_chars, debug=False)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special cases\n",
    "\n",
    "- các kí tự gạch đầu dòng --> box cần nhỏ hơn\n",
    "- check underline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "to_origin = True\n",
    "\n",
    "if to_origin:\n",
    "    vis_img = origin_img.copy()\n",
    "else:\n",
    "    vis_img = beauty_img.copy()\n",
    "\n",
    "for word in words:\n",
    "    _texts = \"\".join(i.get_text() for i in word)\n",
    "    if list(set(_texts))[0] == \"—\" and len(_texts) > 1:\n",
    "        continue\n",
    "\n",
    "    coord = get_xyxy_from_LT(word)  # coord in pdf\n",
    "    coord = convert_coords(coord)\n",
    "    if to_origin:\n",
    "        coord = scale_coord(coord, x_ratio, y_ratio)\n",
    "\n",
    "    x1, y1, x2, y2 = process_special_cases(word, coord, h_scale=0.16, debug=False)\n",
    "    cv2.rectangle(vis_img, (x1, y1), (x2, y2), (255, 0, 0), 1)\n",
    "\n",
    "imshow(vis_img)\n",
    "# imageio.imwrite(\"./data/words.png\", vis_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize line box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis_img = beauty_img.copy()\n",
    "vis_img = origin_img.copy()\n",
    "for line in lines:\n",
    "    coord = line.bbox\n",
    "    coord = convert_coords(coord)\n",
    "    coord = scale_coord(coord, x_ratio, y_ratio)\n",
    "\n",
    "    coord = expand_box(coord, h_scale=0.16)\n",
    "#     coord = process_special_cases(line, coord, h_scale=0.16)\n",
    "    x1, y1, x2, y2 = coord\n",
    "    \n",
    "    cv2.rectangle(vis_img, (x1, y1), (x2, y2), (255, 0, 0), 1)\n",
    "imshow(vis_img)\n",
    "# imageio.imwrite(\"./data/lines.png\", vis_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize char box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_img = beauty_img.copy()\n",
    "# for word in words:\n",
    "#     coords = get_xyxy_from_LT(word)  # coord in pdf\n",
    "#     x1, y1, x2, y2 = convert_coords(coords)\n",
    "#     x1, y1, x2, y2 = map(int, (x1, y1, x2, y2))\n",
    "#     cv2.rectangle(vis_img, (x1, y1), (x2, y2), (255, 0, 0), 1)\n",
    "\n",
    "for word in words:\n",
    "    for char in word:\n",
    "        coord = convert_coords(char)\n",
    "#         x1, y1, x2, y2 = map(int, (x1, y1, x2, y2))\n",
    "        x1, y1, x2, y2 = expand_box(coord)\n",
    "        \n",
    "        cv2.rectangle(vis_img, (x1, y1), (x2, y2), (255, 0, 0), 1)\n",
    "\n",
    "imshow(vis_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyxy2xys(coord, to_int=True):\n",
    "    if to_int:\n",
    "        coord = list(map(int, coord))\n",
    "    x1, y1, x2, y2 = coord\n",
    "    p1 = (x1, y1)\n",
    "    p2 = (x2, y1)\n",
    "    p3 = (x2, y2)\n",
    "    p4 = (x1, y2)\n",
    "    return [p1, p2, p3, p4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word\n",
    "pdf_fps = sorted(glob.glob(os.path.join(\"/media/SUN-ASTERISK\\phan.huy.hoang/My Passport/phanhoang/data/PDF_converter/thuvienphapluat/pdf/*.pdf\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdf_fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocab = \"aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789 \"\n",
    "punc_vocab = '!\"#$%&''()*+,-./:;<=>?@[\\]^_`{|}~'\n",
    "vocab = text_vocab + punc_vocab\n",
    "set_vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 page\n",
    "lines, chars = get_info_LT(paragraphs)\n",
    "# text_chars = \"\".join([i.get_text() for i in chars])\n",
    "text_chars = fix_length_special_cases(chars)\n",
    "words = get_words(chars, text_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    coord = get_xyxy_from_LT(word)  # coord in pdf\n",
    "    coord = convert_coords(coord)\n",
    "    if to_origin:\n",
    "        coord = scale_coord(coord, x_ratio, y_ratio)\n",
    "\n",
    "    x1, y1, x2, y2 = process_special_cases(word, coord, h_scale=0.16, debug=True)\n",
    "    cv2.rectangle(vis_img, (x1, y1), (x2, y2), (255, 0, 0), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_coords(words, xy_ratio=None, return_text=False, to_int=True):\n",
    "    fourpoint_word_coords = []\n",
    "    text_words = []\n",
    "\n",
    "    for word in words:\n",
    "        text = \"\".join(i.get_text()[0] for i in word)\n",
    "        if list(set(text))[0] == \"—\" and len(text) > 1:\n",
    "            continue\n",
    "        \n",
    "        # coord\n",
    "        coord = get_xyxy_from_LT(word)\n",
    "        coord = convert_coords(coord)\n",
    "        if xy_ratio is not None:\n",
    "            x_ratio, y_ratio = xy_ratio\n",
    "            coord = scale_coord(coord, x_ratio, y_ratio)\n",
    "\n",
    "#         coord = expand_box(coord, h_scale=0.16)\n",
    "        coord = process_special_cases(word, coord, h_scale=0.16, debug=False, to_int=to_int)\n",
    "        coord = xyxy2xys(coord, to_int=to_int)\n",
    "        fourpoint_word_coords.append(coord)\n",
    "        \n",
    "        if return_text:\n",
    "            # text\n",
    "            text = \"\".join(i.get_text()[0] for i in word)  # fix\n",
    "#             text = \"\".join(i.get_text() for i in word)  # fix\n",
    "            text_words.append(text)\n",
    "\n",
    "    fourpoint_word_coords = np.array(fourpoint_word_coords)\n",
    "    fourpoint_word_coords = fourpoint_word_coords.transpose(2, 1, 0)\n",
    "    if return_text:\n",
    "        return fourpoint_word_coords, text_words\n",
    "    else:\n",
    "        return fourpoint_word_coords\n",
    "    \n",
    "# fourpoint_word_coords, text_words = get_word_coords(words, return_text=True)\n",
    "# print(fourpoint_word_coords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_coords(chars, xy_ratio=None):\n",
    "    fourpoint_char_coords = []\n",
    "    for i, char in enumerate(chars):\n",
    "        char_text = char.get_text()\n",
    "        if isinstance(char, Space) or len(char_text.strip()) == 0:\n",
    "            continue\n",
    "        coord = convert_coords(char)\n",
    "        if xy_ratio is not None:\n",
    "            x_ratio, y_ratio = xy_ratio\n",
    "            coord = scale_coord(coord, x_ratio, y_ratio)\n",
    "        coord = expand_box(coord, h_scale=0.16)\n",
    "        coord = xyxy2xys(coord, to_int=True)\n",
    "        fourpoint_char_coords.append(coord)\n",
    "\n",
    "    fourpoint_char_coords = np.array(fourpoint_char_coords).transpose(2, 1, 0)\n",
    "    return fourpoint_char_coords\n",
    "\n",
    "# fourpoint_char_coords = get_char_coords(chars)    \n",
    "# print(fourpoint_char_coords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_info(paragraphs, xy_ratio=None):\n",
    "    # 1 page\n",
    "    lines, chars = get_info_LT(paragraphs)\n",
    "#     text_chars = \"\".join([i.get_text() for i in chars])\n",
    "    text_chars = fix_length_special_cases(chars)\n",
    "#     if len(chars) != len(text_chars):  # error\n",
    "#         return\n",
    "    words = get_words(chars, text_chars)\n",
    "    if len(words) == 0:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "#         fourpoint_word_coords, text_words = get_word_coords(words, xy_ratio=xy_ratio)\n",
    "#         fourpoint_char_coords = get_char_coords(chars, xy_ratio=xy_ratio) \n",
    "        fourpoint_word_coords = get_word_coords(words, xy_ratio=xy_ratio, return_text=False, to_int=False)\n",
    "    except Exception as e:\n",
    "        print(\"Error get_page_info: {}\".format(e))\n",
    "        return\n",
    "    \n",
    "    return fourpoint_word_coords\n",
    "#     return fourpoint_word_coords, fourpoint_char_coords, text_words\n",
    "\n",
    "# assert fourpoint_char_coords.shape[2] == sum(len(i) for i in text_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_img_fd = \"/media/SUN-ASTERISK\\phan.huy.hoang/My Passport/phanhoang/data/PDF_converter/thuvienphapluat/pdf_CRAFT_0906\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_content(fd):\n",
    "    shutil.rmtree(fd)\n",
    "    os.mkdir(fd)\n",
    "    \n",
    "# remove_all_content(save_img_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_fps = [os.path.join(tvpl_fd, \"153ded28-4497-4ce0-b5a0-217e9dedf5c9.pdf\")]\n",
    "pdf_fps = sorted(glob.glob(os.path.join(\"/media/SUN-ASTERISK\\phan.huy.hoang/My Passport/phanhoang/data/PDF_converter/thuvienphapluat/pdf/*.pdf\")))\n",
    "print(len(pdf_fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imnames, charBB, wordBB, txt = [], [], [], []\n",
    "valid_ids = {}\n",
    "error_fps = []\n",
    "no_meta_fps = []\n",
    "to_origin = True\n",
    "word_only = True  # save word bounding-box only, ignore char bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf_fp in tqdm_notebook(pdf_fps):\n",
    "\n",
    "    pdf_fn = pdf_fp.strip(\"/\").split(\"/\")[-1]\n",
    "    pdf_id = pdf_fn[:-4]\n",
    "    with open(pdf_fp, \"rb\") as open_pdf:\n",
    "        # pdf to images\n",
    "        try:\n",
    "            images = pdf2image.convert_from_path(pdf_fp)\n",
    "            valid_ids[pdf_id] = {\n",
    "                \"id\": [],\n",
    "                \"img_fp\": None\n",
    "            }\n",
    "\n",
    "            # define pdfminer\n",
    "            rsrcmgr = PDFResourceManager()\n",
    "            laparams = LAParams()\n",
    "            device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "            interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "            parser = PDFParser(open_pdf)\n",
    "            document = PDFDocument(parser)\n",
    "        except Exception:\n",
    "            print(pdf_fp)\n",
    "            continue\n",
    "\n",
    "        # loop for pages\n",
    "        for p, page in enumerate(PDFPage.get_pages(open_pdf)):\n",
    "            try:\n",
    "                interpreter.process_page(page)\n",
    "                paragraphs = device.get_result()\n",
    "\n",
    "                [x_min, y_min, x_max, y_max] = page.mediabox\n",
    "                page_height, page_width = int(abs(y_max - y_min)), int(abs(x_max - x_min))\n",
    "\n",
    "                origin_img = np.array(images[p])\n",
    "                h_origin, w_origin, _ = origin_img.shape\n",
    "                x_ratio = h_origin / page_height\n",
    "                y_ratio = w_origin / page_width\n",
    "                beauty_img = cv2.resize(origin_img.copy(), (page_width, page_height))\n",
    "\n",
    "                if to_origin:\n",
    "                    vis_img = origin_img.copy()\n",
    "                    xy_ratio = (x_ratio, y_ratio)\n",
    "                else:\n",
    "                    vis_img = beauty_img.copy()\n",
    "                    xy_ratio = None\n",
    "                \n",
    "                res = get_page_info(paragraphs, xy_ratio=(x_ratio, y_ratio))\n",
    "                if res is None:\n",
    "                    if pdf_fp not in error_fps:\n",
    "                        error_fps.append(pdf_fp)\n",
    "                    continue\n",
    "                    \n",
    "                if res is False:\n",
    "                    print(\"{}:{} dont have metadata\".format(pdf_fn[:-4], p))\n",
    "                    no_meta_fps.append(pdf_fp)\n",
    "                    continue\n",
    "\n",
    "                random_id = str(uuid.uuid4())\n",
    "                img_fn = \"{}.png\".format(random_id)\n",
    "                save_img_fp = os.path.join(save_img_fd, img_fn)\n",
    "                imageio.imwrite(save_img_fp, vis_img)\n",
    "\n",
    "                valid_ids[pdf_id][\"id\"].append(p)\n",
    "                valid_ids[pdf_id][\"img_fp\"] = save_img_fp\n",
    "                if not word_only:\n",
    "                    fourpoint_word_coords, fourpoint_char_coords, text_words = res\n",
    "                    assert fourpoint_char_coords.shape[2] == sum(len(i) for i in text_words)\n",
    "\n",
    "                    imnames.append(img_fn)\n",
    "                    wordBB.append(fourpoint_word_coords)\n",
    "                    charBB.append(fourpoint_char_coords)\n",
    "                    txt.append(text_words)\n",
    "                    del fourpoint_char_coords, fourpoint_word_coords, text_words\n",
    "                else:\n",
    "                    fourpoint_word_coords = copy.deepcopy(res)\n",
    "\n",
    "                    imnames.append(img_fn)\n",
    "                    wordBB.append(fourpoint_word_coords.astype(np.float16))\n",
    "                    del fourpoint_word_coords\n",
    "\n",
    "            except Exception as e:\n",
    "                print(traceback.format_exc())\n",
    "                print(e)\n",
    "                if pdf_fp not in error_fps:\n",
    "                    error_fps.append(pdf_fp)\n",
    "                continue\n",
    "        \n",
    "            gc.collect()\n",
    "            \n",
    "        if len(valid_ids[pdf_id][\"id\"]) == 0:\n",
    "            del valid_ids[pdf_id]\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_dir = \"/media/SUN-ASTERISK\\phan.huy.hoang/My Passport/phanhoang/data/PDF_converter/thuvienphapluat/custom_synthtext/annotations/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordBB2 = [i.astype(np.float16) for i in wordBB]\n",
    "# imnames = [i.split(\"/\")[-1] for i in imnames]\n",
    "\n",
    "with open(os.path.join(dump_dir, \"train_wordBB.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(wordBB2, f)\n",
    "          \n",
    "with open(os.path.join(dump_dir, \"train_imnames.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(imnames, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_id = random.choice(range(len(imnames)))\n",
    "random_id = 0\n",
    "\n",
    "img_fp = os.path.join(save_img_fd, imnames[random_id])\n",
    "assert os.path.exists(img_fp)\n",
    "img = cv2.imread(img_fp)[:, :, ::-1]\n",
    "polys = wordBB[random_id]\n",
    "print(polys.shape)\n",
    "\n",
    "vis_img = img.copy()\n",
    "for poly in polys.transpose(2, 1, 0):\n",
    "    poly = poly.astype(int)\n",
    "    cv2.polylines(vis_img, [poly], True, (255, 0, 0), thickness=1)\n",
    "\n",
    "imshow(vis_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hoangph36_env",
   "language": "python",
   "name": "hoangph36_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
